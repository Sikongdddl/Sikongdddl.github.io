<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>paper on Sikongdddl的博客</title><link>https://Sikongdddl.github.io/categories/paper/</link><description>Recent content in paper on Sikongdddl的博客</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Fri, 01 Sep 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://Sikongdddl.github.io/categories/paper/index.xml" rel="self" type="application/rss+xml"/><item><title>光电计算读书笔记</title><link>https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</link><pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate><guid>https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</guid><description>&lt;h2 id="对光电计算领域的认识">对光电计算领域的认识
&lt;/h2>&lt;p>读到的这四篇工作各有千秋 实验部分没有仔细看 因为一方面这已经是很顶级的工作了，另一方面只要效果出来不输给传统方法，就是行得通的&lt;/p>
&lt;p>其中三篇是用光电的手段实现了一些传统方式的模型结构或算法：经典深度神经网络，卷积和VAE。另一篇是在光电的可扩展性方面做出了一些尝试，用基础元件的堆叠来实现“可编程”的光电模型。&lt;/p>
&lt;p>光电计算要做的事情是在硬件层面使用光学技术（衍射等）来实现电子元件支持的算符和操作，从而让依赖电子元件的算法摆脱电子元件性能瓶颈。例如宾夕法尼亚大学的那篇工作，用部分光学元件实现了一个做图片分类的神经网络，处理一张图片的速度竟然是0.57ns，这是任何运行在计算机上的程序都无法做到的，因为0.57ns只相当于一个2GHZ的CPU的一个时钟周期。&lt;/p>
&lt;p>paper里看到一段话很贴切：&lt;/p>
&lt;blockquote>
&lt;p>Optical computing uses photons instead of electrons for computation, and this process can overcome the inherent limitations of electronics and improve the energy efficiency, processing speed and computational throughput by orders of magnitude.&lt;/p>&lt;/blockquote>
&lt;p>这是一个技术上已经比较成熟的领域，和纯电子计算相比，光电计算有一些非常显著的优势：（当然这完全是两个领域，对比没有太大意义，但也因为完全是两个领域，才能取得难以想象的突破）&lt;/p>
&lt;ul>
&lt;li>速度极快 这是由于传输信息的底层方式不同&lt;/li>
&lt;li>更适合处理光学信息（如处理光纤中的数据）&lt;/li>
&lt;li>单元体积极小&lt;/li>
&lt;/ul>
&lt;p>它也有一些比较明显的劣势：&lt;/p>
&lt;ul>
&lt;li>可扩展性非常差 以前总开玩笑说代码改不了，焊死了，这回是真焊死了。调整模型在光电元件上十分困难 也许需要重新做一张芯片。其中一篇增加可扩展性的工作思路是使用基本元件进行堆叠和编程，但这种基本元件的量产难度也依然存在。我们也许很难像蚀刻芯片那样来批量化的制造光电元件，这可能让许多算力的解放只能发生在实验室中，发生在某一个细致的领域内，通用的光电计算机好像已经诞生（金贤敏老师的工作），但普及仍然有很大距离。&lt;/li>
&lt;li>开发调试成本。做硬件肯定比写代码要更加困难。正如那个著名的笑话所讲，硬件工程师的耗材可以堆满一间实验室，而软件工程师的耗材在自动售货机中就可以全部找到。&lt;/li>
&lt;/ul>
&lt;p>这个领域内评价一篇工作好坏的指标大概有以下三个：&lt;/p>
&lt;ul>
&lt;li>实现效果 用光电实现的对应技术的效果至少要对标传统方式实现的技术 能超越就更棒了&lt;/li>
&lt;li>速度 这个一般是远远超过传统方法的&lt;/li>
&lt;li>可扩展性 这个应该是最重要的&lt;/li>
&lt;/ul>
&lt;p>总的来说这是个让我感到振奋的研究领域，也有可能是读的这几篇都是nature science级别的工作，这些工作肯定很让人振奋（笑）&lt;/p>
&lt;h2 id="潜在的可以开展的方向">潜在的可以开展的方向
&lt;/h2>&lt;h3 id="大模型">大模型
&lt;/h3>&lt;p>说实话我第一篇（宾夕法尼亚大学做DNN那篇）读到一半，大概意识到这个领域想做什么后，第一反应就是transformer能不能用这种形式创造出来，哪怕量产的时间和经济成本再高，我也觉得这是件有意义的事情。&lt;/p>
&lt;p>因为GPT系的大模型就是decoder-only的transformer的堆叠，传统方式训练它消耗的人力和物力已经是天文数字了。也许是因为对领域的了解依然不足，我现在读完之后依然觉得如果光电可以做出transformer元件，像reconfigurable DPU那样去堆叠它来进行大模型的训练是一件有意义的事。&lt;/p>
&lt;p>VAE能做，卷积能做，甚至RNN都能做出个类似的，transformer应该也能做吧。
如果能解决单个元件的量产和元件之间的通信，以及中间权重的存储这三个问题，那也许用光电实现的大模型落地就只是时间问题了。我不信他比现在的大模型更贵（&lt;/p>
&lt;p>也许任何一个科研人员都不希望看到某个领域最优秀的成果像NLP的大模型这样，沦为一个要投入巨大的人力物力才有收益的怪物。&lt;/p>
&lt;h3 id="可扩展性">可扩展性
&lt;/h3>&lt;p>读到一些光电或者纯光学的硬件实现的深度学习概念让我哭笑不得 因为我从软件的角度出发思考这个问题得到的答案和实际的解决办法相去甚远。&lt;/p>
&lt;p>可编程本身是一个很珍贵的属性。我可以在集成开发环境里实现各种各样的天马行空的想法，但它的前提条件是硬件支持已经到位了。因此在硬件层面做改变时，很多我这种做软件的人完全想象不到的困难就会产生。&lt;/p>
&lt;p>我记得有篇工作详细的解释了，怎么利用材料的跳变做出了一个以电压作为自变量的Relu函数，这让我很受打击。因为这意味着前几年那些面向深度学习模型的炼丹操作完全无法在光电这边实现，连一个Relu都这么难做，调整模型的成本绝对不是放在几张显卡上跑几个小时那么简单。&lt;/p>
&lt;p>我难以想象这个领域工作的可扩展性会以什么样的方式提升，开发出一种基本元件（像DPU那种）并把它进行堆叠来实现编程也许是一个好主意，但自由度依然没有真正的编程那么高。我期待光电领域的汇编语言或高级语言出现。&lt;/p>
&lt;p>当然，像可扩展性这种软件层面的提升都要随着硬件的改变而改变。当年的汇编程序员是比谁代码写的更短的，而不是比谁代码写得更好。如果存储资源一直十分昂贵的话，高级语言或许都不会诞生。&lt;/p>
&lt;h2 id="相关知识储备">相关知识储备
&lt;/h2>&lt;p>硬件也分传统的电子硬件和光学硬件，我对电子硬件多少还有些了解，对光学硬件就基本没有接触过了。&lt;/p>
&lt;blockquote>
&lt;p>软件方面&lt;/p>&lt;/blockquote>
&lt;p>在交大软工这边造了三年轮子，让我有足够的对新技术的热情和学习能力，支持我在较短时间内学会一门新的开发语言或技术。很多课程需要的知识都是以前没有接触过，但在同时忙很多事的情况下，一个学期内掌握并出活的。&lt;/p>
&lt;p>做过一些大模型结合强化学习的工作，相关proposal会贴在最后。在科大讯飞出差期间写过面向大模型的插件，比较了解GPT系的大模型：包括发展历史，原理和调用方法。&lt;/p>
&lt;p>玩过的东西很杂很乱，像hugo stack，普罗米修斯等等， 一些诸如服务器管理、实验室博客这种杂七杂八的事情我应该上手都会比较快。另外我可以完整的写一套包括前端 后端 消息中间件和数据库的网站，并发数100以下没有问题，可以帮忙造一些轮子提高大家的工作效率（笑）。也许一些仿真工作（如果有的话）也可以我来做 我用过multisim、simulink这种东西，虽然不知道光学的硬件要怎么仿真，但应该学起来也比较快。&lt;/p>
&lt;blockquote>
&lt;p>硬件方面&lt;/p>&lt;/blockquote>
&lt;p>我的理论知识就停留在致远的数理方法了，学过一些简单的复变函数，积分变换和数学物理方程，但我并不了解信号与系统 数字信号处理等等知识。&lt;/p>
&lt;p>ipads实验室过硬的操作系统课程让我对电子计算硬件有一些了解。我们面向一个运行在qemu中的，微内核的chcore操作系统写过lab，实现了诸如系统启动，内存分配，IPC，线程调度，文件系统等等功能，算是初步接触过面向电子计算硬件的开发工作。&lt;/p>
&lt;h2 id="要学习的技能">要学习的技能
&lt;/h2>&lt;p>我对光学硬件应该算是一窍不通了（笑），上次接触还是大一做物理实验，这个要重点进修一下&lt;/p>
&lt;p>以及很多时候我解决问题的第一想法都是从软件层面出发的，把很多硬件支持当成理所当然的事，并且不怎么在乎试错成本。online judge提交几十次，看到模型第一反应是调参&amp;hellip;这都是并不适合做类似读到的这几篇论文的这种研究的。&lt;/p>
&lt;h2 id="论文内容简述">论文内容简述
&lt;/h2>&lt;p>按发刊时间顺序排列&lt;/p>
&lt;h3 id="large-scale-neuromorphic-optoelectronic-computing-with-a-reconfigurable-diffractive-processing-unit">Large-scale neuromorphic optoelectronic computing with a reconfigurable diffractive processing unit
&lt;/h3>&lt;p>这篇主要关注可扩展性 提出了实现不同ANN架构的DPU集群神经网络（衍射计算单元） 并提出了一种自适应的训练方法（类似退火 在第k次训练中更新第一层到第k层的权重 随着k增加到总层数 训练完成） 我以前读到过的这篇工作：&lt;/p>
&lt;blockquote>
&lt;p>ROD: Reception-aware Online Distillation for Sparse Graphs&lt;/p>&lt;/blockquote>
&lt;p>其中的方法和这篇提出的训练方法非常相似 这是一篇知识蒸馏的工作 他的第k次teacher model是前k个model的汇集&lt;/p>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-1.png"
width="1645"
height="950"
srcset="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-1_hu_e2b9caff86fc831b.png 480w, https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-1_hu_c3c903d83a15dfa1.png 1024w"
loading="lazy"
alt="Method"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="415px"
>&lt;/p>
&lt;p>a图意思应该是支持不同架构的DNN 可以直接更换模块化的输入输出单元 可编程以改变功能&lt;/p>
&lt;p>输入这边对应不同的diffractive module有对应的information coding 把输入进来的电信号转化成光信号&lt;/p>
&lt;p>输出这边有对应的光场求和（光学的线性计算）和非线性激活 他这里全部都是光学计算&lt;/p>
&lt;p>b图是光电元件实现 这里的电子计算只包括数据流的存储和缓存 全部的计算工作都是光学完成的&lt;/p>
&lt;p>权重由衍射调制来控制 求和直接用光场求和就可以&lt;/p>
&lt;p>后面三张图是基于DPU的不同神经网络架构 DMD是个数字透镜 SLM是光调制器 这两个可以实现输入（电信号-光信号） CMOS是个传感器 实现线性和非线性运算 所以一个DPU可以作为光学神经网络里的一个神经元&lt;/p>
&lt;p>c图是把DPU叠起来当神经网络用 网络中每一层都是一个DPU&lt;/p>
&lt;p>d图是更大型的 中间加入了一些feature map&lt;/p>
&lt;p>e图类似RNN&lt;/p>
&lt;p>实验是在c图那种架构的神经网络上做的MNIST和RNN架构上的人类动作识别 后者已经是相当可以的数据集了&lt;/p>
&lt;h3 id="parallel-convolutional-processing-using-an-integrated-photonic-tensor-core">Parallel convolutional processing using an integrated photonic tensor core
&lt;/h3>&lt;p>光量子我之前有一点点了解&lt;/p>
&lt;p>致远的科研体验课程zirc中的物理方向金贤敏老师是做光量子的
那门课抢的很激烈 我选老师的时候没选中他 后来在致远学者计划那边聊天的时候 听他讲过一个小时的光量子&lt;/p>
&lt;p>这篇是21年的工作 那个时候深度神经网络卷生卷死 模型越来越杂 这里认为瓶颈在卷积上也很合理&lt;/p>
&lt;p>他使用光学硬件来支持神经网络的初衷并不是加速经典神经网络的那些线性运算和非线性激活，而是加速卷积&lt;/p>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-3.png"
width="1686"
height="1135"
srcset="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-3_hu_c6a7b12b73f6b5b4.png 480w, https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-3_hu_636329dd1ce865a9.png 1024w"
loading="lazy"
alt="Method"
class="gallery-image"
data-flex-grow="148"
data-flex-basis="356px"
>&lt;/p>
&lt;p>光源能够提供不同精确频率的光波作为输入 用WDM保证不同的光信号可以集成到一个光纤上 并且可以用DEMUX无损的分离 这样才能计算和储存结果&lt;/p>
&lt;p>其实也不一定多快，但是它是并行运算的 每一个source vector可以映射成光梳上的一个波段 波段的自由度很高 这样可以一次让非常非常多的source vector和卷积核做运算 并一次存储非常非常多的结果&lt;/p>
&lt;p>CNN中的source vector可以来自于图片的不同区域 因此这种并行运算很方便的可以提升卷积这一步的运算速度 而且在参数量很高的时候这个提升是巨大的&lt;/p>
&lt;p>这个硬件部分只做卷积 其它的东西还是电子计算 所以要先实验卷积的情况 再实验模型的效果&lt;/p>
&lt;p>只改变卷积的计算方式，在MNIST上，这种光学卷积的CNN和传统CNN达到了相差无几的效果 卷积本身的正确性也很OK&lt;/p>
&lt;p>但是为什么没有看到关于提速情况的实验呢 这个矩阵也只是个9*4的，完全体现不出并行的效果&lt;/p>
&lt;p>后面附录里看到作者说矩阵大了之后精度会下降，而且下降程度和矩阵大小成正比&lt;/p>
&lt;h3 id="an-on-chip-photonic-deep-neural-network-for-image-classification">An on-chip photonic deep neural network for image classification
&lt;/h3>&lt;p>做图像识别的神经网络&lt;/p>
&lt;p>线性计算部分由光学来做 非线性激活部分还是光电的&lt;/p>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image.png"
width="1618"
height="1021"
srcset="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image_hu_33aaea80a2242f3c.png 480w, https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image_hu_4e9b10e190723b9d.png 1024w"
loading="lazy"
alt="Method"
class="gallery-image"
data-flex-grow="158"
data-flex-basis="380px"
>&lt;/p>
&lt;p>a b: 没有对神经网络的工作原理做任何修改&lt;/p>
&lt;p>c d: 但是硬件支持整个换了一套 这边展开看看：&lt;/p>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-2.png"
width="1584"
height="924"
srcset="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-2_hu_26411b279af90d8d.png 480w, https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-2_hu_71c5775b45fc6729.png 1024w"
loading="lazy"
alt="hardware implementation"
class="gallery-image"
data-flex-grow="171"
data-flex-basis="411px"
>&lt;/p>
&lt;p>引脚衰减器 高中自己研究半导体导电的时候找到过类似PN结的说法 好像是说晶胞里的电子缺位可以视为正电荷 多余电子可以视为负电荷 通过PN结来传递电信号 由于一边是P另一边是N 所以这种导电是单向的&lt;/p>
&lt;p>不知道和这里的PN有什么联系 感觉半导体原理应该都大差不差吧&lt;/p>
&lt;p>电流越大衰减越强 基本成正比&lt;/p>
&lt;p>衰减后的信号交给硅锗二极管 做线性运算（其实就是加权的加法）后交给非线性激活元件 这里是个RELU 实现原理在f g图里 事实上是为了保证速度不要在这里出现瓶颈，用光电硬件的物理特性凑出了RELU的效果&lt;/p>
&lt;p>芯片本身怎么烧的没看 实在是看不懂一点（笑）&lt;/p>
&lt;p>可扩展性比较差（模型是物理上焊死的）&lt;/p>
&lt;p>效果跟传统的神经网络差不太多（手写字母二分类四分类识别实验） 但是速度快（0.57ns 和电子计算方式已经不在一个数量级了）&lt;/p>
&lt;h3 id="photonic-unsupervised-learning-variational-autoencoder-for-high-throughput-and-low-latency-image-transmission">Photonic unsupervised learning variational autoencoder for high-throughput and low-latency image transmission
&lt;/h3>&lt;p>光电硬件实现的VAE 做image transmission又快又准 真没想过VAE还能干这个&lt;/p>
&lt;p>图片传输的瓶颈在电信号和光信号的转换上&lt;/p>
&lt;p>说的有点对 我们是不是把科技树点歪了，为什么数据用光纤传输，但是输入输出非要用电子元件处理呢（笑&lt;/p>
&lt;p>当前的全光学神经网络都是做分类任务的 图片传输还需要模型有重建能力 所以像VAE GAN这种网络才能做光纤这里的图像传输&lt;/p>
&lt;p>这事儿倒也有人在做 但延迟非常高 而且做不到全光端到端&lt;/p>
&lt;p>全光版本的VAE&lt;/p>
&lt;ul>
&lt;li>输入光信号&lt;/li>
&lt;li>latent space光信号&lt;/li>
&lt;li>输出图像&lt;/li>
&lt;/ul>
&lt;p>比CPU快四个数量级，传输错误率同比下降57%&lt;/p>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-4.png"
width="945"
height="1071"
srcset="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-4_hu_82a50034ed501c4f.png 480w, https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-4_hu_d6b33d76dcaf15dc.png 1024w"
loading="lazy"
alt="实在是好图 一眼就懂了"
class="gallery-image"
data-flex-grow="88"
data-flex-basis="211px"
>&lt;/p>
&lt;p>A：光纤传输latent space的光学VAE&lt;/p>
&lt;p>B：真正的光纤&lt;/p>
&lt;p>C：General mode下，输入数字信号（把图片转化成二进制）时的encode-decode是无偏的 因为是光学信号 已经可以进行图片传输&lt;/p>
&lt;p>D：Data-specific mode下，输入灰度图片信号时效果也还OK 专门用来进行高通量图片传输&lt;/p>
&lt;h2 id="大模型强化学习proposal">大模型强化学习proposal
&lt;/h2>&lt;p>&lt;img src="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-5.png"
width="890"
height="1151"
srcset="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-5_hu_4554b7459de59973.png 480w, https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-5_hu_f919ac8b534a90fe.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="77"
data-flex-basis="185px"
>&lt;/p>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-6.png"
width="888"
height="1143"
srcset="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-6_hu_68f7a9a75e639810.png 480w, https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-6_hu_bf2a3b7973292e69.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="77"
data-flex-basis="186px"
>&lt;/p>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-7.png"
width="885"
height="1146"
srcset="https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-7_hu_c50448645896e088.png 480w, https://Sikongdddl.github.io/p/%E5%85%89%E7%94%B5%E8%AE%A1%E7%AE%97%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/image-7_hu_65c90f52730e28.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="77"
data-flex-basis="185px"
>&lt;/p></description></item><item><title>星火营干货及近日进展</title><link>https://Sikongdddl.github.io/p/%E6%98%9F%E7%81%AB%E8%90%A5%E5%B9%B2%E8%B4%A7%E5%8F%8A%E8%BF%91%E6%97%A5%E8%BF%9B%E5%B1%95/</link><pubDate>Sun, 20 Aug 2023 00:00:00 +0000</pubDate><guid>https://Sikongdddl.github.io/p/%E6%98%9F%E7%81%AB%E8%90%A5%E5%B9%B2%E8%B4%A7%E5%8F%8A%E8%BF%91%E6%97%A5%E8%BF%9B%E5%B1%95/</guid><description>&lt;h2 id="为什么这篇不是论文笔记">为什么这篇不是论文笔记
&lt;/h2>&lt;p>科大讯飞这边日程排的又满又不科学 很浪费时间
加上最近接到了电话
这俩事都值得汇报一下 都挺大的 就没去读论文（笑）&lt;/p>
&lt;h2 id="讯飞星火大模型相关">讯飞星火大模型相关
&lt;/h2>&lt;p>去年12月项目开始，5月6日发布13B参数量模型，8月15日发布65B参数量模型&lt;/p>
&lt;p>效果低于GPT3.5 但是低的不多 比国内其他大模型还是好一些的&lt;/p>
&lt;p>借了华为一部分算力资源支持 加上讯飞常年做输入法和语音识别，训练数据质量很高，清洗数据方法也很成熟 所以虽然仓促但是能做出来&lt;/p>
&lt;h2 id="星火营相关">星火营相关
&lt;/h2>&lt;p>成员基本是全国985211的硕士博士生 本科生偏少
很多人都是带着活过来这边 一边被老板push一边做这边的事（笑）&lt;/p>
&lt;blockquote>
&lt;p>8.15-8.17上大课&lt;/p>&lt;/blockquote>
&lt;p>除了第一天讲过一些大模型相关的算法和知识之外 都是讲座性质的介绍课程 比较无聊（笑）&lt;/p>
&lt;p>讲了大模型的发展沿革（BERT系和GPT系）&lt;/p>
&lt;p>transformer诞生后&lt;/p>
&lt;p>BERT系做一些参数量相对没有那么夸张的大模型 做encoder only，识别任务更多一些 必须fine-tune 预训练方式是mask后完形填空 或根据上一句话写出下一句话 标注成本高&lt;/p>
&lt;p>GPT系做参数量很夸张的大模型（GPT3就有175B） decoder only，做生成任务更多一些&lt;/p>
&lt;p>GPT系能做出来原因主要是：&lt;/p>
&lt;ul>
&lt;li>transformer的Encoder本身叠不深 不全是钱的问题 到100B这种参数量的encoder-only训练不起来&lt;/li>
&lt;li>GPT系泛化能力比BERT系好（不用finetune）而且预训练任务更简单&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>8.18“测评”&lt;/p>&lt;/blockquote>
&lt;p>分成7人小组 想prompt 想评价体系横向比较讯飞大模型和GPT3.5的情况 代码比较简单 一些基本的API调用 但是工作量安排不合理 对prompt量要求较大 全员加班 下班时间11：00-2：00不定 我们十二点搞好的&lt;/p>
&lt;blockquote>
&lt;p>8.19-8.24做项目&lt;/p>&lt;/blockquote>
&lt;p>我和宋的选题是给大模型做插件 技术上也不复杂 总体上要做的是截获用户的prompt，润色后丢给第三方处理（如询问天气）或交给大模型，获得更丰富有效的回答 主要考察想象力（笑）&lt;/p>
&lt;blockquote>
&lt;p>奖金：（全组奖金 税前）&lt;/p>&lt;/blockquote>
&lt;p>16组进8&lt;/p>
&lt;p>4-8名20000&lt;/p>
&lt;p>2-3名50000&lt;/p>
&lt;p>1名100000&lt;/p>
&lt;p>组队运气不好 本来最厉害的大哥（交大18级硕士毕业，去北京工作了几年回来读博）被老板叫回去了 除了宋能帮到比较大的忙以外 其它几位朋友基础都比较差&lt;/p>
&lt;p>现在项目是我在架构和管理 进度比较乐观 工作强度也不大 估计明天就能做完 但应该不考虑奖金的事情了 真能拿到奖金的话回来就充公（笑）&lt;/p>
&lt;p>也算是有收获的 感觉自己和别人（尤其是比较菜的朋友）交流的能力显著增强 也慢慢明白要怎么和更厉害的人相处才让双方都舒服&lt;/p>
&lt;h2 id="关于戴院士组里来交大就职的老师">关于戴院士组里来交大就职的老师
&lt;/h2>&lt;p>写到这才发现我不知道她叫什么名字（笑）&lt;/p>
&lt;p>8月19日晚收到了面试电话 简单交流了下&lt;/p>
&lt;p>交流内容大概包括：&lt;/p>
&lt;ul>
&lt;li>关于推免资格（我一接到电话就全交代了 她看简历并不知道第六学期的事 但她还是坚持把电话打完了（笑））&lt;/li>
&lt;li>代码能力（她说我挺能写码的 是她想找的类型）&lt;/li>
&lt;li>为什么挺能写码但是GPA不高（致远+最后一学期专业课组队失利 她听我说组队就明白了）&lt;/li>
&lt;li>之前的研究方向和科研经历&lt;/li>
&lt;li>交大的保研流程常识（主要是我在介绍 好像她并不清楚）&lt;/li>
&lt;li>为什么这么晚了还没确定接收（于是我又说了一遍我最近这几个月的叠加态）&lt;/li>
&lt;li>问王老师有几个名额（实话实说了我）&lt;/li>
&lt;li>她和我一开始一样 也没想明白我怎么能报到她这种24年入职的老师 我说报戴院士 挂大老板 她有点没反应过来愣了一下 可能之前没考虑过这种展开（笑）&lt;/li>
&lt;li>研究方向偏硬件会不会不喜欢（介绍了硬件层面加速算法的一些知识 以及说那边做纯算法的太卷了 博士得发十篇 做做硬件还没那么卷 我说挺感兴趣 如果这玩意能让大模型更好训练就好了）&lt;/li>
&lt;li>会不会顾虑年轻老师资源少（我说喜欢被push 年轻老师在学生身上花的时间多）&lt;/li>
&lt;/ul>
&lt;p>第一次电话结束后约十分钟又来了一通电话 问我为什么不去杨老师这边的其它小老板那里（马老师 晏老师和沈老师） 看起来还是没有完全理解交大的流程 我解释说被CS挂简历了 去不了计算机系&lt;/p>
&lt;p>最后的结论：&lt;/p>
&lt;ul>
&lt;li>她说这波有好多学生 所以现在不能确定下来（这句话翻来覆去说了好几次 婉拒了属于是） 让我确认拿到推免资格后再联系她一次&lt;/li>
&lt;/ul></description></item><item><title>Reinforcement Learning from Passive Data via Latent Intentions</title><link>https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/</link><pubDate>Sun, 13 Aug 2023 00:00:00 +0000</pubDate><guid>https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/</guid><description>&lt;img src="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/bookface.jpg" alt="Featured image of post Reinforcement Learning from Passive Data via Latent Intentions " />&lt;h2 id="abstract">Abstract
&lt;/h2>&lt;p>大多数真实视频对RL来说都比较难办 因为不知道怎么定义value function和action label&lt;/p>
&lt;p>这些被称为passive data 被动可能被动在无法落入现成的评价体系里&lt;/p>
&lt;p>这里提出model intentions：猜下一步action后变化情况的概率分布。&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;How likely am I to see __ if I act to do __ from this state&amp;rdquo;&lt;/p>&lt;/blockquote>
&lt;p>和dreamer有点像属于是&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/dibyaghosh/icvf_release" target="_blank" rel="noopener"
>https://github.com/dibyaghosh/icvf_release&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>非常耐心地讲了一下RL为什么难以处理真实数据 因为缺乏好用的评价指标&lt;/p>
&lt;p>这篇工作引入了intension space Z,以此作为value function的主要参考。&lt;/p>
&lt;p>agent在环境中怎么互动和怎么探索有轮子，工作主要集中在：&lt;/p>
&lt;ul>
&lt;li>预测&lt;/li>
&lt;li>完成latent space训练后，这些intension数据在action中的使用情况&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image.png"
width="826"
height="194"
srcset="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image_hu_233a564bdd78e028.png 480w, https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image_hu_a1cc6f174966f90d.png 1024w"
loading="lazy"
alt="示意图"
class="gallery-image"
data-flex-grow="425"
data-flex-basis="1021px"
>&lt;/p>
&lt;p>这个老师比较诙谐&lt;/p>
&lt;p>图比较好理解，latent space的内容是“对未来假设进行一些动作会发生的情况的预测”，拿这个和st st+1凑一个value function。&lt;/p>
&lt;h2 id="formulation">Formulation
&lt;/h2>&lt;p>马尔可夫链 没啥新鲜东西&lt;/p>
&lt;h2 id="methodrl-using-passive-data">Method：RL using passive Data
&lt;/h2>&lt;h3 id="icvf-intension-conditioned-value-function">ICVF intension-conditioned value function
&lt;/h3>&lt;p>传统的RL学一个Q function state-action对
我们学一个Z function，用intension代替state,用outcome代替action&lt;/p>
&lt;p>以前是状态随着action改变 现在是猜一个intension，拿一个outcome检验，再猜下一个&lt;/p>
&lt;p>那这个和dreamer相差非常大了（笑）&lt;/p>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-1.png"
width="549"
height="89"
srcset="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-1_hu_908cd8793085826f.png 480w, https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-1_hu_69d75c29b32118aa.png 1024w"
loading="lazy"
alt="Alt text"
class="gallery-image"
data-flex-grow="616"
data-flex-basis="1480px"
>&lt;/p>
&lt;p>他这里可以把各种任务的reward做个期望分布&lt;/p>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-2.png"
width="384"
height="104"
srcset="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-2_hu_215c53b452351b34.png 480w, https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-2_hu_e4319b2b4ad472f4.png 1024w"
loading="lazy"
alt="Alt text"
class="gallery-image"
data-flex-grow="369"
data-flex-basis="886px"
>&lt;/p>
&lt;p>这样可以让基于intension的value function和具体的任务解耦 所以他说这个预训练方法适用于所有任务&lt;/p>
&lt;h3 id="learning-representations-from-icvf">learning representations from ICVF
&lt;/h3>&lt;p>这个Value function黑盒的 怎么说明它对接下来的RL任务起到了多少作用呢？&lt;/p>
&lt;ul>
&lt;li>转化成的state representation必须能比较容易的从model里得到&lt;/li>
&lt;li>面对一个具体的task，这个state representation必须对具体的reward有用&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-3.png"
width="529"
height="81"
srcset="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-3_hu_36ef8173956c8538.png 480w, https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-3_hu_27ae0d3b3c0ad98c.png 1024w"
loading="lazy"
alt="Alt text"
class="gallery-image"
data-flex-grow="653"
data-flex-basis="1567px"
>&lt;/p>
&lt;p>先做个数学处理 把value function改成向量形式&lt;/p>
&lt;p>这是个线性无关的 三个向量分别代表着state（es） ，过了一层intension后的state（中间那一坨 这是1993年一个工作的表示方式）以及实际的最后的state（es+）&lt;/p>
&lt;p>所以我们学三个网络来表征value function&lt;/p>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-4.png"
width="554"
height="79"
srcset="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-4_hu_95b2a36c1034a1dc.png 480w, https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-4_hu_ba5b5e7c446a1e8.png 1024w"
loading="lazy"
alt="Alt text"
class="gallery-image"
data-flex-grow="701"
data-flex-basis="1683px"
>&lt;/p>
&lt;p>现在就满足了我们的这两个需求：&lt;/p>
&lt;ul>
&lt;li>好表示 这都解耦了当然好表示&lt;/li>
&lt;li>有用 他说他们证明了当任务确定下来的时候，他们的representation线性接近任何一个具体的task&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-5.png"
width="829"
height="436"
srcset="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-5_hu_5a3354e1aabc3c9b.png 480w, https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-5_hu_96ff7f63d2d84b7a.png 1024w"
loading="lazy"
alt="一个奇怪的放缩"
class="gallery-image"
data-flex-grow="190"
data-flex-basis="456px"
>&lt;/p>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-6.png"
width="698"
height="545"
srcset="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-6_hu_5d890d260e00b38b.png 480w, https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-6_hu_980d49596879a03a.png 1024w"
loading="lazy"
alt="Alt text"
class="gallery-image"
data-flex-grow="128"
data-flex-basis="307px"
>&lt;/p>
&lt;p>没看懂（笑）&lt;/p>
&lt;h3 id="learning-icvf-from-passive-data">learning ICVF from passive data
&lt;/h3>&lt;p>最后一块拼图&lt;/p>
&lt;p>证明了ICVF有用和怎么用之后 问题就是ICVF怎么来了&lt;/p>
&lt;p>这里只是提出了一种task情境下的passive data-ICVF 是一个multi-linear 多重线性&lt;/p>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-7.png"
width="630"
height="168"
srcset="https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-7_hu_68b2775c7657e56a.png 480w, https://Sikongdddl.github.io/p/reinforcement-learning-from-passive-data-via-latent-intentions/image-7_hu_51c3ea09df0d573c.png 1024w"
loading="lazy"
alt="Alt text"
class="gallery-image"
data-flex-grow="375"
data-flex-basis="900px"
>&lt;/p>
&lt;p>很无聊 就是从（st,at,st+1）的buffer里拿一堆数据丢进这个所谓的ICVF&lt;/p>
&lt;h3 id="实验">实验
&lt;/h3>&lt;p>明天补上（笑） 他们自己搭了环境 不是传统的offline环境 还挺有趣的&lt;/p></description></item><item><title>HIQL：Offline Goal-Conditioned RL with Latent States as Actions</title><link>https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/</link><pubDate>Fri, 04 Aug 2023 00:00:00 +0000</pubDate><guid>https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/</guid><description>&lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/bookface.png" alt="Featured image of post HIQL：Offline Goal-Conditioned RL with Latent States as Actions" />&lt;h2 id="just-deliver-something">Just deliver something
&lt;/h2>&lt;p>在2023年3月到5月这段时间我进行过一些subgoal-based RL的研究和尝试，比较遗憾没有什么结果。&lt;/p>
&lt;p>老板推荐，偶然发现了这位来自UC伯克利的小日子老师做了很多subgoal的工作。&lt;/p>
&lt;p>这我不得抱起来狠狠拜读！&lt;/p>
&lt;h2 id="abstract">Abstract
&lt;/h2>&lt;p>如果能给subgoal组织成良好的结构，先找近的，一步一步往远走就容易一些&lt;/p>
&lt;p>组织方式和director很像 一层manager一层worker。&lt;/p>
&lt;p>manager从state得到latent representation作为subgoal，worker实现subgoal&lt;/p>
&lt;p>特殊的地方在于这个产生了manager和worker的value function不含action&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://seohong.me/projects/hiql/" target="_blank" rel="noopener"
>https://seohong.me/projects/hiql/&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>offline go-conditioned RL问题：&lt;/p>
&lt;ul>
&lt;li>（state, goal）-&amp;gt; value function学不出来&lt;/li>
&lt;li>offline数据源太多，合并进入传统RL方法有困难&lt;/li>
&lt;/ul>
&lt;p>说了一个可能导致sparse reward难学的原因
当goal很远时，好的策略和不好的策略几乎看不出区别，因为任何错误都可以在之后的过程中得到修正（理论上），这就导致非常难学到一个从头到尾都正确的action series&lt;/p>
&lt;p>解决办法跟director类似&lt;/p>
&lt;ul>
&lt;li>IQL从offline buffer里学一个policy&lt;/li>
&lt;li>一个policy变俩，manager+worker&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-1.png"
width="915"
height="699"
srcset="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-1_hu_cac95df66e558afb.png 480w, https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-1_hu_1c0a4f1460664e73.png 1024w"
loading="lazy"
alt="IQL&amp;#43;manager&amp;#43;worker"
class="gallery-image"
data-flex-grow="130"
data-flex-basis="314px"
>&lt;/p>
&lt;p>g:final reward&lt;/p>
&lt;p>s:state&lt;/p>
&lt;p>z:subgoal&lt;/p>
&lt;p>a:action&lt;/p>
&lt;p>k:每k步一个subgoal&lt;/p>
&lt;p>IQL:V(s,g)&lt;/p>
&lt;p>manager:s,g-&amp;gt;z&lt;/p>
&lt;p>worker:s,z-&amp;gt;a&lt;/p>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-2.png"
width="1130"
height="720"
srcset="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-2_hu_888e522efe121415.png 480w, https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-2_hu_2e87442e4d0735e6.png 1024w"
loading="lazy"
alt="value function"
class="gallery-image"
data-flex-grow="156"
data-flex-basis="376px"
>&lt;/p>
&lt;p>虽然不是同一个policy，但是同一个value function，可能会不好学&lt;/p>
&lt;p>所以虽然同一个value function，但是不是同一个policy，fine-tune了一下&lt;/p>
&lt;p>manager需要学的是怎么每K步做一个subgoal&lt;/p>
&lt;p>worker需要学的是怎么到达眼前这个subgoal&lt;/p>
&lt;p>亮点是IQL和manager不关心action，所以能应对offline的大量数据
而且不需要experts trajectories&lt;/p>
&lt;h2 id="preliminaries">Preliminaries
&lt;/h2>&lt;p>&lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-3.png"
width="1984"
height="568"
srcset="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-3_hu_60c0e4abef5fe659.png 480w, https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-3_hu_2294c457b2d02752.png 1024w"
loading="lazy"
alt="setting"
class="gallery-image"
data-flex-grow="349"
data-flex-basis="838px"
>&lt;/p>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-9.png"
width="2326"
height="603"
srcset="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-9_hu_91a1efcb58acb6f6.png 480w, https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-9_hu_9e2011c8336c0985.png 1024w"
loading="lazy"
alt="IQL"
class="gallery-image"
data-flex-grow="385"
data-flex-basis="925px"
>&lt;/p>
&lt;p>IQL是他们组2022年一篇ICLR poster的工作：论文里讲的挺细，最后的结论是IQL和一般的Q-learning比，不需要那么关注action，但是我确实看不懂他这个Loss哪来的。&lt;/p>
&lt;p>于是决定去知乎，文化先跟初中生同步一下&lt;/p>
&lt;ul>
&lt;li>
&lt;p>OfflineRL本来的Loss：&lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-5.png"
width="1461"
height="248"
srcset="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-5_hu_3db978a1ac0c6d01.png 480w, https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-5_hu_b9eb3d3e3d360b92.png 1024w"
loading="lazy"
alt="Alt text"
class="gallery-image"
data-flex-grow="589"
data-flex-basis="1413px"
> 取让s最接近s&amp;rsquo;的action的Q过来。问题是定义域外的action也能算出一个Q，把这个Loss搞废了&lt;/p>
&lt;/li>
&lt;li>
&lt;p>改进的Loss：&lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-6.png"
width="1279"
height="180"
srcset="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-6_hu_e939391ab5061a34.png 480w, https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-6_hu_c47cc6684ef2b8c3.png 1024w"
loading="lazy"
alt="Alt text"
class="gallery-image"
data-flex-grow="710"
data-flex-basis="1705px"
>不取最大，不做query，因此定义域外的action不会参与loss计算&lt;/p>
&lt;/li>
&lt;li>
&lt;p>引入调参手段：&lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-7.png"
width="1304"
height="128"
srcset="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-7_hu_16f9f33742f148e2.png 480w, https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-7_hu_802d6cfd83d7da77.png 1024w"
loading="lazy"
alt="Alt text"
class="gallery-image"
data-flex-grow="1018"
data-flex-basis="2445px"
>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>把s&amp;rsquo;的Q分成两个值函数构造：&lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-8.png"
width="1290"
height="333"
srcset="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-8_hu_de50adb5db88255d.png 480w, https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-8_hu_be27848692c7c29c.png 1024w"
loading="lazy"
alt="Alt text"
class="gallery-image"
data-flex-grow="387"
data-flex-basis="929px"
>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最后把Qlearning拿走，放弃学习action与state之间的关系，就得到了HIQL这篇里的东西啦&lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-10.png"
width="1694"
height="129"
srcset="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-10_hu_2dd8826d1287ba57.png 480w, https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-10_hu_68bf05d59e955e7f.png 1024w"
loading="lazy"
alt="Alt text"
class="gallery-image"
data-flex-grow="1313"
data-flex-basis="3151px"
> &lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-11.png"
width="1619"
height="124"
srcset="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-11_hu_e43f401fcd86137d.png 480w, https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-11_hu_84b853b3d54db753.png 1024w"
loading="lazy"
alt="Alt text"
class="gallery-image"
data-flex-grow="1305"
data-flex-basis="3133px"
> &lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-12.png"
width="1124"
height="84"
srcset="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-12_hu_66854e59e710f3ac.png 480w, https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-12_hu_78ed75df09ef2b77.png 1024w"
loading="lazy"
alt="Alt text"
class="gallery-image"
data-flex-grow="1338"
data-flex-basis="3211px"
>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="hierarchical-policy-structure">Hierarchical policy structure
&lt;/h2>&lt;h3 id="作者眼中sparse-reward困难的原因信噪比">作者眼中sparse reward困难的原因：信噪比
&lt;/h3>&lt;ul>
&lt;li>final reward很远的时候，不同的state之间输出的value差别会很小 这导致一些错误的action是可以得到纠正的 所以很难分辨action的质量&lt;/li>
&lt;li>更糟糕的是，在难以分辨的有效值以外充满大量的噪声
&lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-13.png"
width="1045"
height="556"
srcset="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-13_hu_af891187c6474818.png 480w, https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-13_hu_af7ba5a1529ecea4.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="187"
data-flex-basis="451px"
>&lt;/li>
&lt;/ul>
&lt;h3 id="hierarchical-policy-structure-1">hierarchical policy structure
&lt;/h3>&lt;p>重复描述Introduction中对应的这一部分&lt;/p>
&lt;h3 id="toy环境上的理论证明">toy环境上的理论证明
&lt;/h3>&lt;p>&lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-14.png"
width="748"
height="303"
srcset="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-14_hu_c93e589b6b5ed2.png 480w, https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-14_hu_996e86c02f4ad7e8.png 1024w"
loading="lazy"
alt="toy环境"
class="gallery-image"
data-flex-grow="246"
data-flex-basis="592px"
>
这个环境下Value function就是单纯的-|s-g|&lt;/p>
&lt;ul>
&lt;li>因为状态转移概率是1所以没有期望 因为只有一维所以不需要累加/累乘
这个环境下，subgoal被人为控制的越来越远
那么如果我们不做subgoal，考虑的范围就只有左右两格。每K步做一次subgoal 考虑的范围就是左右K格（笑）&lt;/li>
&lt;/ul>
&lt;p>这里作者附录里给出了理论证明：&lt;/p>
&lt;ul>
&lt;li>找到了在有subgoal情况下，这个toy环境中决策错误概率的理论上界&lt;/li>
&lt;li>证明了他们方法的manager和worker犯错概率都要比flat策略更低&lt;/li>
&lt;/ul>
&lt;p>如果我刚学完概统我肯定能把这段看懂，可惜已经过去快两年了（笑）&lt;/p>
&lt;h2 id="methodhierarchical-implicit-q-learning">Method：Hierarchical Implicit Q-learning
&lt;/h2>&lt;p>非常简短。。。&lt;/p>
&lt;h3 id="策略一分为二">策略一分为二
&lt;/h3>&lt;p>&lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-15.png"
width="1768"
height="253"
srcset="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-15_hu_8bf85ddfe97e51b5.png 480w, https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-15_hu_4fb4f05deb235a59.png 1024w"
loading="lazy"
alt="Alt text"
class="gallery-image"
data-flex-grow="698"
data-flex-basis="1677px"
>
上面那个是manager学怎么产生subgoal&lt;/p>
&lt;p>下面那个是worker学怎么接近下一个subgoal&lt;/p>
&lt;p>A其实就是前后两个Value function的差 具体怎么减在Introduction的这一部分里介绍过了&lt;/p>
&lt;p>&lt;img src="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-16.png"
width="2375"
height="859"
srcset="https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-16_hu_6c17397178fd5c64.png 480w, https://Sikongdddl.github.io/p/hiqloffline-goal-conditioned-rl-with-latent-states-as-actions/image-16_hu_ba687e33d9122c70.png 1024w"
loading="lazy"
alt="Alt text"
class="gallery-image"
data-flex-grow="276"
data-flex-basis="663px"
>&lt;/p>
&lt;p>训练完value function之后分出manager和worker两个policy&lt;/p>
&lt;h3 id="subgoal的表征方式">subgoal的表征方式
&lt;/h3>&lt;p>直接用state的一部分当subgoal&lt;/p></description></item></channel></rss>